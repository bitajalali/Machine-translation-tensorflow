{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb","timestamp":1580864385977}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"EbAG2kPQ2bxt","outputId":"6b0a5a16-f8f5-4423-ab71-b9d477539247","executionInfo":{"status":"ok","timestamp":1581131717234,"user_tz":-210,"elapsed":10269,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":276}},"source":["# !wget http://www.manythings.org/anki/deu-eng.zip\n","# !unzip /content/deu-eng.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-02-08 03:15:09--  http://www.manythings.org/anki/deu-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:3033::6818:6dc4, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7747747 (7.4M) [application/zip]\n","Saving to: ‘deu-eng.zip’\n","\n","deu-eng.zip         100%[===================>]   7.39M  18.0MB/s    in 0.4s    \n","\n","2020-02-08 03:15:14 (18.0 MB/s) - ‘deu-eng.zip’ saved [7747747/7747747]\n","\n","Archive:  /content/deu-eng.zip\n","  inflating: deu.txt                 \n","  inflating: _about.txt              \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tnxXKDjq3jEL","outputId":"6925baef-cac8-4a52-8e47-24223997b251","executionInfo":{"status":"ok","timestamp":1581131719386,"user_tz":-210,"elapsed":12406,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cnt85HhF3vIK"},"source":["path_to_file = \"/content/deu.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rd0jw-eC3jEh"},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","      if unicodedata.category(c) != 'Mn')\n","\n","\n","def preprocess_sentence(w):\n","  w = unicode_to_ascii(w.lower().strip())\n","\n","  # creating a space between a word and the punctuation following it\n","  # eg: \"he is a boy.\" => \"he is a boy .\"\n","  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","\n","  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","\n","  w = w.rstrip().strip()\n","\n","  # adding a start and an end token to the sentence\n","  # so that the model know when to start and stop predicting.\n","  w = '<start> ' + w + ' <end>'\n","  return w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHn4Dct23jEm"},"source":["# 1. Remove the accents\n","# 2. Clean the sentences\n","# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n","def create_dataset(path, num_examples):\n","  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","  \n","  return word_pairs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTbSbBz55QtF","outputId":"d2ec7742-bd0b-4efa-ee1e-b137fe3b531d","executionInfo":{"status":"ok","timestamp":1581131734433,"user_tz":-210,"elapsed":27408,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":["word_pairs = create_dataset(path_to_file, None)\n","en,deu,_ = zip(*word_pairs)\n","print(en[-1])\n","print(deu[-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n","<start> ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht , dass ein mensch nur gelegenheit hat , mit ein paar hundert anderen bekannt zu sein , von denen ihm nur ein dutzend oder weniger nahesteht , darunter hochstens ein oder zwei freunde , dann erahnt man eingedenk der millionen einwohner dieser welt leicht , dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist . <end>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OmMZQpdO60dt"},"source":["def max_length(tensor):\n","  return max(len(t) for t in tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIOn8RCNDJXG"},"source":["def tokenize(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","  lang_tokenizer.fit_on_texts(lang)\n","\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","  return tensor, lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAY9k49G3jE_"},"source":["def load_dataset(path, num_examples=None):\n","  # creating cleaned input, output pairs\n","  word_pairs = create_dataset(path, num_examples)\n","  targ_lang, inp_lang,_ = zip(*word_pairs)\n","\n","  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","\n","  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GOi42V79Ydlr"},"source":["### Limit the size of the dataset to experiment faster (optional)\n","\n","Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"]},{"cell_type":"code","metadata":{"id":"cnxC7q-j3jFD"},"source":["# Try experimenting with the size of that dataset\n","num_examples = 30000\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n","\n","# Calculate max_length of the target tensors\n","max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QILQkOs3jFG","outputId":"8b9beff8-016e-4359-85d3-621790293b75","executionInfo":{"status":"ok","timestamp":1581131737938,"user_tz":-210,"elapsed":30858,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Creating training and validation sets using an 80-20 split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n","\n","# Show length\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["24000 24000 6000 6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lJPmLZGMeD5q"},"source":["def convert(lang, tensor):\n","  for t in tensor:\n","    if t!=0:\n","      print (\"%d ----> %s\" % (t, lang.index_word[t]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXukARTDd7MT","outputId":"d8991508-8471-48e3-8a30-5ec088036eae","executionInfo":{"status":"ok","timestamp":1581131737946,"user_tz":-210,"elapsed":30844,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":331}},"source":["print (\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print ()\n","print (\"Target Language; index to word mapping\")\n","convert(targ_lang, target_tensor_train[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input Language; index to word mapping\n","1 ----> <start>\n","5 ----> ich\n","50 ----> werde\n","10 ----> es\n","3222 ----> zulassen\n","3 ----> .\n","2 ----> <end>\n","\n","Target Language; index to word mapping\n","1 ----> <start>\n","5 ----> i\n","27 ----> ll\n","998 ----> allow\n","9 ----> it\n","3 ----> .\n","2 ----> <end>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rgCLkfv5uO3d"},"source":["### Create a tf.data dataset"]},{"cell_type":"code","metadata":{"id":"TqHsArVZ3jFS"},"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qc6-NK1GtWQt","outputId":"fcfa9e49-473a-4d86-acca-ad8853f9c924","executionInfo":{"status":"ok","timestamp":1581131738305,"user_tz":-210,"elapsed":31185,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["example_input_batch, example_target_batch = next(iter(dataset))\n","example_input_batch.shape, example_target_batch.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 14]), TensorShape([64, 10]))"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"nZ2rI24i3jFg"},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","    super(Encoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output, state = self.gru(x, initial_state = hidden)\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.enc_units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60gSVh05Jl6l","outputId":"82aea0f2-3074-453e-8f4b-d2f8277ad93d","executionInfo":{"status":"ok","timestamp":1581131750021,"user_tz":-210,"elapsed":42885,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Encoder output shape: (batch size, sequence length, units) (64, 14, 1024)\n","Encoder Hidden state shape: (batch size, units) (64, 1024)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"umohpBN2OM94"},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # hidden shape == (batch_size, hidden size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","    # we are doing this to perform addition to calculate the score\n","    hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(values) + self.W2(hidden_with_time_axis)))\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k534zTHiDjQU","outputId":"d0781bac-3e16-4520-9241-6ccc6ceccd3d","executionInfo":{"status":"ok","timestamp":1581131751458,"user_tz":-210,"elapsed":44302,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Attention result shape: (batch size, units) (64, 1024)\n","Attention weights shape: (batch_size, sequence_length, 1) (64, 14, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yJ_B3mhW3jFk"},"source":["class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","    super(Decoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.dec_units = dec_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.lstm1 = tf.keras.layers.LSTM(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.lstm2 = tf.keras.layers.LSTM(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # used for attention\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","  def call(self, x, hidden, enc_output):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the lstm\n","    output = self.lstm1(x)\n","    output = self.lstm2(output)\n","    state = output[1]\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.reshape(output[0], (-1, output[0].shape[2]))\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(output)\n","\n","    return x, state, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5UY8wko3jFp","outputId":"39513013-f120-409c-c3ee-a731ec28fbef","executionInfo":{"status":"ok","timestamp":1581132405408,"user_tz":-210,"elapsed":814,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Decoder output shape: (batch_size, vocab size) (64, 4559)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_ch_71VbIRfK"},"source":["## Define the optimizer and the loss function"]},{"cell_type":"code","metadata":{"id":"WmTHr5iV3jFr"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMVWzzsfNl4e"},"source":["## Checkpoints (Object-based saving)"]},{"cell_type":"code","metadata":{"id":"Zj8bXQTgNwrF"},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sC9ArXSsVfqn"},"source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","    dec_hidden = enc_hidden\n","\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(1, targ.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      loss += loss_function(targ[:, t], predictions)\n","\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, variables)\n","\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddefjBMa3jF0","outputId":"b7174cbf-cccf-4372-8445-bcc59412bae5","executionInfo":{"status":"ok","timestamp":1581132880286,"user_tz":-210,"elapsed":467324,"user":{"displayName":"hemen zandi","photoUrl":"","userId":"07645378304220230421"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","    batch_loss = train_step(inp, targ, enc_hidden)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","  # saving (checkpoint) the model every 2 epochs\n","  if (epoch + 1) % 2 == 0:\n","    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 4.9233\n","Epoch 1 Batch 100 Loss 2.2294\n","Epoch 1 Batch 200 Loss 1.9156\n","Epoch 1 Batch 300 Loss 1.5874\n","Epoch 1 Loss 1.9973\n","Time taken for 1 epoch 61.12903904914856 sec\n","\n","Epoch 2 Batch 0 Loss 1.5785\n","Epoch 2 Batch 100 Loss 1.3743\n","Epoch 2 Batch 200 Loss 1.3413\n","Epoch 2 Batch 300 Loss 1.3234\n","Epoch 2 Loss 1.3396\n","Time taken for 1 epoch 45.71835494041443 sec\n","\n","Epoch 3 Batch 0 Loss 1.0908\n","Epoch 3 Batch 100 Loss 1.0223\n","Epoch 3 Batch 200 Loss 1.0939\n","Epoch 3 Batch 300 Loss 0.9758\n","Epoch 3 Loss 1.0298\n","Time taken for 1 epoch 44.35605549812317 sec\n","\n","Epoch 4 Batch 0 Loss 0.7675\n","Epoch 4 Batch 100 Loss 0.7919\n","Epoch 4 Batch 200 Loss 0.7433\n","Epoch 4 Batch 300 Loss 0.7637\n","Epoch 4 Loss 0.7635\n","Time taken for 1 epoch 45.49629187583923 sec\n","\n","Epoch 5 Batch 0 Loss 0.5662\n","Epoch 5 Batch 100 Loss 0.5470\n","Epoch 5 Batch 200 Loss 0.5724\n","Epoch 5 Batch 300 Loss 0.5340\n","Epoch 5 Loss 0.5407\n","Time taken for 1 epoch 44.89355278015137 sec\n","\n","Epoch 6 Batch 0 Loss 0.3341\n","Epoch 6 Batch 100 Loss 0.3960\n","Epoch 6 Batch 200 Loss 0.4098\n","Epoch 6 Batch 300 Loss 0.4170\n","Epoch 6 Loss 0.3694\n","Time taken for 1 epoch 45.6908814907074 sec\n","\n","Epoch 7 Batch 0 Loss 0.1954\n","Epoch 7 Batch 100 Loss 0.2166\n","Epoch 7 Batch 200 Loss 0.3247\n","Epoch 7 Batch 300 Loss 0.2228\n","Epoch 7 Loss 0.2505\n","Time taken for 1 epoch 44.476372480392456 sec\n","\n","Epoch 8 Batch 0 Loss 0.1221\n","Epoch 8 Batch 100 Loss 0.2292\n","Epoch 8 Batch 200 Loss 0.1987\n","Epoch 8 Batch 300 Loss 0.2079\n","Epoch 8 Loss 0.1707\n","Time taken for 1 epoch 45.16103792190552 sec\n","\n","Epoch 9 Batch 0 Loss 0.1280\n","Epoch 9 Batch 100 Loss 0.1462\n","Epoch 9 Batch 200 Loss 0.1438\n","Epoch 9 Batch 300 Loss 0.1123\n","Epoch 9 Loss 0.1211\n","Time taken for 1 epoch 44.38347363471985 sec\n","\n","Epoch 10 Batch 0 Loss 0.0783\n","Epoch 10 Batch 100 Loss 0.0561\n","Epoch 10 Batch 200 Loss 0.0966\n","Epoch 10 Batch 300 Loss 0.1197\n","Epoch 10 Loss 0.0934\n","Time taken for 1 epoch 45.11469388008118 sec\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mU3Ce8M6I3rz"},"source":["## Translate\n","\n","* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n","* Stop predicting when the model predicts the *end token*.\n","* And store the *attention weights for every time step*.\n","\n","Note: The encoder output is calculated only once for one input."]},{"cell_type":"code","metadata":{"id":"EbQpyYs13jF_"},"source":["def evaluate(sentence):\n","  attention_plot = np.zeros((max_length_targ, max_length_inp))\n","\n","  sentence = preprocess_sentence(sentence)\n","\n","  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","  inputs = tf.convert_to_tensor(inputs)\n","\n","  result = ''\n","\n","  hidden = [tf.zeros((1, units))]\n","  enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","  dec_hidden = enc_hidden\n","  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","\n","  for t in range(max_length_targ):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","\n","    # storing the attention weights to plot later on\n","    attention_weights = tf.reshape(attention_weights, (-1, ))\n","    attention_plot[t] = attention_weights.numpy()\n","\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","    result += targ_lang.index_word[predicted_id] + ' '\n","\n","    if targ_lang.index_word[predicted_id] == '<end>':\n","      return result, sentence, attention_plot\n","\n","    # the predicted ID is fed back into the model\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  return result, sentence, attention_plot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5hQWlbN3jGF"},"source":["# function for plotting the attention weights\n","def plot_attention(attention, sentence, predicted_sentence):\n","  fig = plt.figure(figsize=(10,10))\n","  ax = fig.add_subplot(1, 1, 1)\n","  ax.matshow(attention, cmap='viridis')\n","\n","  fontdict = {'fontsize': 14}\n","\n","  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sl9zUHzg3jGI"},"source":["import nltk\n","def translate(sentence,true_translate):\n","  result, sentence, attention_plot = evaluate(sentence)\n","\n","  print('Input: %s' % (sentence))\n","  print('Predicted translation: {}'.format(result))\n","\n","  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n","  print(\"BLEU : \", nltk.translate.bleu_score.sentence_bleu([result.split(' ')],true_translate))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n250XbnjOaqP"},"source":["## Restore the latest checkpoint and test"]},{"cell_type":"code","metadata":{"id":"UJpT9D5_OgP6"},"source":["# restoring the latest checkpoint in checkpoint_dir\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WrAM0FDomq3E"},"source":["translate(u\"Es ist wirklich kalt hier.\",\"It's really cold here.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zSx2iM36EZQZ"},"source":["translate(u'das ist mein Leben.',\"that is my life.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3LLCx3ZE0Ls"},"source":["translate(u\"Sind sie noch zu Hause?\",\"Are you still at home?\")"],"execution_count":null,"outputs":[]}]}